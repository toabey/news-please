# This is a HJSON-File, so comments and so on can be used! See https://hjson.org/
# Furthermore this is first of all the actual config file, but as default just filled with examples.
{
  # Every URL has to be in an array-object in "base_urls".
  # The same URL in combination with the same crawler may only appear once in this array.
  "base_urls" : [

    {
      # Start crawling from faz.net
      "url": "http://timesofindia.indiatimes.com/city/delhi",

      "ignore_regex" : "(/city/delhi)|(/articlelist/*curpg=)",

      # Overwrite the default crawler and use th RecursiveCrawler instead
      "crawler": "RecursiveCrawler",
    },

    {
      # Start crawling from faz.net
      "url": "http://www.hindustantimes.com/delhi/",

      "ignore_regex" : "(/delhi/)",

      # Overwrite the default crawler and use th RecursiveCrawler instead
      "crawler": "RecursiveCrawler",
      },
      {
      # Start crawling from faz.net
      "url": "http://indianexpress.com/section/cities/delhi/",

      "ignore_regex" : "(/cities/delhi/)",

      # Overwrite the default crawler and use th RecursiveCrawler instead
      "crawler": "RecursiveCrawler",
      }

  ]
}
